# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZqSIInBTcEi_hvm4sIVSGAza6mNp5rLd
"""

import json
import time
import os
import random
import google.generativeai as genai

# üîπ Replace with your API key
API_KEY = 'API'
genai.configure(api_key=API_KEY)
def translate_text(text, target_language, max_retries=5):
    """Translates text using Gemini 1.5 Pro Latest with exponential backoff and jitter."""
    model = genai.GenerativeModel("gemini-1.5-pro-latest")
    prompt = f"Translate the following text to {target_language}. Maintain the original meaning and context: '{text}'"

    wait_time = 10  # Initial wait time (seconds)

    for attempt in range(1, max_retries + 1):
        try:
            response = model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            error_message = str(e).lower()
            print(f"‚ùå Error on attempt {attempt}: {e}")

            # Check for quota errors or network issues
            if "quota" in error_message or "rate limit" in error_message:
                jitter = random.uniform(0, 5)  # Add random jitter (0-5 sec)
                print(f"‚è≥ Rate limit hit. Retrying in {wait_time + jitter:.2f} seconds...")
                time.sleep(wait_time + jitter)
                wait_time *= 2  # Exponential backoff (10s ‚Üí 20s ‚Üí 40s ‚Üí 80s)
            elif "network" in error_message or "unavailable" in error_message:
                print(f"üåê Network issue detected. Retrying in {wait_time} seconds...")
                time.sleep(wait_time)
            else:
                return text  # Return original text if it's a different error

    print("‚ö† Max retries reached. Returning original text.")
    return text  # Return original text after all retries

def translate_json(input_path, output_dir, target_language, batch_size=5, start_batch=206):
    """Translates a JSON file in batches, starting from batch number 200."""
    with open(input_path, 'r', encoding='utf-8') as file:
        data = json.load(file)

    total_batches = (len(data) + batch_size - 1) // batch_size  # Total batches
    print(f"üìä Total batches: {total_batches} (Starting from batch {start_batch})")

    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists

    translated_data = []

    for i in range(start_batch * batch_size, len(data), batch_size):
        batch = data[i:i + batch_size]
        batch_num = i // batch_size + 1  # Calculate batch number

        batch_translated = []
        for item in batch:
            translated_item = {
                "question": translate_text(item['question'], target_language),
                "answer": translate_text(item['answer'], target_language)
            }
            batch_translated.append(translated_item)

        translated_data.extend(batch_translated)

        # Save each batch separately
        batch_filename = os.path.join(output_dir, f"batch_{batch_num}.json")
        with open(batch_filename, 'w', encoding='utf-8') as batch_file:
            json.dump(batch_translated, batch_file, ensure_ascii=False, indent=2)

        print(f"‚úÖ Processed and saved batch {batch_num}/{total_batches}")
        time.sleep(5)  # Short delay to avoid API overload

    # Save final combined output
    final_output_path = os.path.join(output_dir, "translated_full.json")
    with open(final_output_path, 'w', encoding='utf-8') as file:
        json.dump(translated_data, file, ensure_ascii=False, indent=2)

    print(f"üéâ Translation to {target_language} completed! All batches saved in {output_dir}")

# ‚úÖ Run Translation from Batch you want to run
input_path = "full.json"
output_dir = "./translated_batches"
translate_json(input_path, output_dir, "Hindi", start_batch=0)

import os
import json

def combine_json_files(output_dir):
    """Combines all batch JSON files in the output directory into a single JSON file."""
    combined_data = []

    # List all JSON files in the directory
    for filename in os.listdir(output_dir):
        if filename.endswith(".json") and filename.startswith("batch_"):
            batch_path = os.path.join(output_dir, filename)
            with open(batch_path, 'r', encoding='utf-8') as batch_file:
                batch_data = json.load(batch_file)
                combined_data.extend(batch_data)  # Merge batch data into combined_data

    # Save the combined data into the final JSON file
    final_output_path = os.path.join(output_dir, "translated_full.json")
    with open(final_output_path, 'w', encoding='utf-8') as final_file:
        json.dump(combined_data, final_file, ensure_ascii=False, indent=2)

    print(f"üéâ All batch files have been merged into {final_output_path}")

# Usage after translation:
input_path = r'C:\Users\songj\OneDrive\Desktop\De-fine-tuning-Unlearning-Multilingual-Language-Models\DB\TOFU\full.json'
korean_output_path = r'C:\Users\swetha\OneDrive\Desktop\De-fine-tuning-Unlearning-Multilingual-Language-Models\DB\TOFU\full_kor_gemini.json'
hindi_output_path = r'C:\Users\swetha\OneDrive\Desktop\De-fine-tuning-Unlearning-Multilingual-Language-Models\DB\TOFU\full_hindi_gemini.json'
# translate_json(input_path, output_dir, "Hindi", start_batch=200)

# Now combine the JSONs into one file
combine_json_files(output_dir)
