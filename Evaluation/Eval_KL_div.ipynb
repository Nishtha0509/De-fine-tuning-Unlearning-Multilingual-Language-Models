{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f18220f-6ca6-4f39-a224-601162e724c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 15:45:55,039 - INFO - Starting model evaluation script (Batched Generation, Quantization Disabled).\n",
      "2025-04-11 15:45:55,039 - WARNING - Bitsandbytes quantization is disabled. Ensure you have enough GPU memory.\n",
      "2025-04-11 15:45:55,040 - INFO - Generation Batch Size: 32\n",
      "2025-04-11 15:45:55,040 - INFO - CUDA is available. Using GPU: NVIDIA A100-SXM4-80GB\n",
      "2025-04-11 15:45:55,042 - INFO - Found 9 JSON files to process.\n",
      "2025-04-11 15:45:55,042 - INFO - Loading model: Unlearned_LLaMA_KD from ../Unlearning/kd_unlearned_model (Quantization Disabled)\n",
      "2025-04-11 15:45:55,042 - INFO - Loading full model from: ../Unlearning/kd_unlearned_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9493eb0d9d491e9f59e0114b94f179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 15:46:16,318 - INFO - Loading tokenizer from model path: ../Unlearning/kd_unlearned_model\n",
      "2025-04-11 15:46:16,596 - ERROR - Error loading model Unlearned_LLaMA_KD: 'NoneType' object has no attribute 'eval'\n",
      "2025-04-11 15:46:16,597 - ERROR - Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1873001/3713157732.py\", line 113, in load_model_and_tokenizer\n",
      "    model.eval() # Set model to evaluation mode\n",
      "    ^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'eval'\n",
      "\n",
      "2025-04-11 15:46:16,754 - ERROR - Skipping evaluation for model Unlearned_LLaMA_KD due to loading failure.\n",
      "2025-04-11 15:46:16,755 - INFO - Cleaning up resources for model: Unlearned_LLaMA_KD\n",
      "2025-04-11 15:46:16,936 - INFO - Resources cleaned up for model: Unlearned_LLaMA_KD\n",
      "2025-04-11 15:46:21,938 - INFO - Script finished.\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re # Regular expression import added\n",
    "import gc\n",
    "import time\n",
    "import traceback\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Any, Literal\n",
    "from io import StringIO\n",
    "import contextlib\n",
    "import math # For ceil\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "from bert_score import score as bert_score_calculate # Import specific function for clarity\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Model Configuration ---\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    name: str\n",
    "    model_path: str\n",
    "    is_local: bool = True\n",
    "    model_type: Literal[\"causal\", \"encoder\", \"encoder-decoder\"] = \"causal\"\n",
    "    is_adapter_model: bool = False\n",
    "    base_model_path_for_adapter: Optional[str] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.is_adapter_model and not self.base_model_path_for_adapter:\n",
    "            raise ValueError(f\"Model '{self.name}' is marked as adapter model, but 'base_model_path_for_adapter' is not provided.\")\n",
    "        if self.is_adapter_model and self.base_model_path_for_adapter == self.model_path:\n",
    "            raise ValueError(f\"For adapter model '{self.name}', 'base_model_path_for_adapter' cannot be the same as 'model_path'.\")\n",
    "\n",
    "# --- Define Unlearned Model to Evaluate ---\n",
    "MODEL_CONFIGS = [\n",
    "    ModelConfig(\n",
    "        name=\"Unlearned_LLaMA_KD\",\n",
    "        #model_path=\"./kd_unlearned_model\",\n",
    "        model_path = \"../Unlearning/kd_unlearned_model\",\n",
    "        is_local=True,\n",
    "        is_adapter_model=False\n",
    "    )\n",
    "]\n",
    "\n",
    "# --- Evaluation Configuration ---\n",
    "DATA_DIRECTORIES = [\n",
    "    \"/scratch/dshajkum/De-fine-tuning-Unlearning-Multilingual-Language-Models/DB/TOFU/train\",\n",
    "    \"/scratch/dshajkum/De-fine-tuning-Unlearning-Multilingual-Language-Models/DB/TOFU/unlearning\"\n",
    "]\n",
    "OUTPUT_DIR = \"/scratch/dshajkum/De-fine-tuning-Unlearning-Multilingual-Language-Models/Evaluation/TOFU_Evaluation_Results_epoch5\"\n",
    "MAX_NEW_TOKENS = 150\n",
    "BERT_SCORE_MODEL_TYPE = \"bert-base-multilingual-cased\"\n",
    "GENERATION_BATCH_SIZE = 32\n",
    "BATCH_SIZE_BERT_SCORE = 16\n",
    "\n",
    "# --- Helper Functions (load_model_and_tokenizer, generate_answers_batch, calculate_bert_score_batch remain the same) ---\n",
    "\n",
    "def load_model_and_tokenizer(config: ModelConfig, device: torch.device):\n",
    "    \"\"\"Loads the model and tokenizer based on the configuration without bitsandbytes quantization.\"\"\"\n",
    "    logger.info(f\"Loading model: {config.name} from {config.model_path} (Quantization Disabled)\")\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    load_path = config.model_path\n",
    "    base_load_path = config.base_model_path_for_adapter if config.is_adapter_model else config.model_path\n",
    "\n",
    "    try:\n",
    "        if config.is_adapter_model:\n",
    "            logger.info(f\"Loading base model for adapter from: {config.base_model_path_for_adapter}\")\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    config.model_path,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    trust_remote_code=True\n",
    "                ).to(device)\n",
    "\n",
    "            logger.info(f\"Loading adapter weights from: {config.model_path}\")\n",
    "            model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                config.model_path,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "            tokenizer_load_path = config.base_model_path_for_adapter\n",
    "            logger.info(f\"Loading tokenizer from base model path: {tokenizer_load_path}\")\n",
    "        else:\n",
    "             logger.info(f\"Loading full model from: {config.model_path}\")\n",
    "             base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    config.model_path,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    trust_remote_code=True\n",
    "                ).to(device)\n",
    "             tokenizer_load_path = config.model_path\n",
    "             logger.info(f\"Loading tokenizer from model path: {tokenizer_load_path}\")\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_load_path, trust_remote_code=True)\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            logger.warning(\"Tokenizer does not have a pad token. Setting pad_token to eos_token.\")\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            # Ensure model config pad_token_id is also set, handling potential PeftModel wrapping\n",
    "            model_to_configure = model\n",
    "            if hasattr(model, 'base_model'): # If it's a PeftModel, configure the base model\n",
    "                model_to_configure = model.base_model\n",
    "            if hasattr(model_to_configure, 'config') and hasattr(model_to_configure.config, 'pad_token_id'):\n",
    "                model_to_configure.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        logger.info(f\"Model '{config.name}' and tokenizer loaded successfully.\")\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model {config.name}: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        del model, tokenizer\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        return None, None\n",
    "\n",
    "# --- BATCHED Generation Function ---\n",
    "def generate_answers_batch(model, tokenizer, questions: List[str], max_new_tokens: int) -> List[Optional[str]]:\n",
    "    \"\"\"Generates answers for a batch of questions using the model.\"\"\"\n",
    "    prompts = [f\"Question: {q}\\nAnswer:\" for q in questions]\n",
    "    generated_answers = []\n",
    "\n",
    "    try:\n",
    "        # Determine the device of the model (important if using device_map='auto')\n",
    "        try:\n",
    "            model_device = next(model.parameters()).device\n",
    "        except StopIteration:\n",
    "            logger.warning(\"Could not determine model device, assuming CPU.\")\n",
    "            model_device = torch.device(\"cpu\") # Fallback\n",
    "        except AttributeError: # Handle cases where model might not have parameters directly (e.g. error during load)\n",
    "             logger.warning(\"Model does not seem to have parameters, assuming CPU.\")\n",
    "             model_device = torch.device(\"cpu\")\n",
    "\n",
    "        # Tokenize the batch of prompts\n",
    "        inputs = tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,       # Pad sequences to the longest in the batch\n",
    "            truncation=True,\n",
    "            max_length=512 # Set a reasonable max_length for input to avoid excessive padding\n",
    "        ).to(model_device)\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode each generated sequence in the batch\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        # logger.debug(f\"Batch generation - Input length: {input_length}, Output shape: {outputs.shape}\")\n",
    "\n",
    "        for i in range(outputs.shape[0]):\n",
    "            generated_ids = outputs[i, input_length:]\n",
    "            try:\n",
    "                # logger.debug(f\"Batch item {i} - Generated IDs: {generated_ids}\")\n",
    "                # raw_decoded = tokenizer.decode(generated_ids, skip_special_tokens=False)\n",
    "                # logger.debug(f\"Batch item {i} - Raw Decoded: '{raw_decoded}'\")\n",
    "                answer = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "                # logger.debug(f\"Batch item {i} - Final Answer: '{answer}'\")\n",
    "                generated_answers.append(answer)\n",
    "            except Exception as decode_err:\n",
    "                 logger.error(f\"Error decoding answer for question batch item {i}: {decode_err}\")\n",
    "                 generated_answers.append(None) # Append None if decoding fails\n",
    "\n",
    "        # Ensure the number of answers matches the number of questions\n",
    "        if len(generated_answers) != len(questions):\n",
    "            logger.error(f\"Mismatch in generated answers count ({len(generated_answers)}) vs questions count ({len(questions)}). Filling missing with None.\")\n",
    "            generated_answers.extend([None] * (len(questions) - len(generated_answers)))\n",
    "\n",
    "        return generated_answers\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during batch generation for questions starting with '{questions[0][:50]}...': {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return [None] * len(questions)\n",
    "\n",
    "\n",
    "def calculate_bert_score_batch(predictions: List[str], references: List[str], device: torch.device) -> Optional[Dict[str, List[float]]]:\n",
    "    \"\"\"Calculates BERTScore (P, R, F1) for batches of predictions and references.\"\"\"\n",
    "    if not predictions or not references:\n",
    "        logger.warning(\"Empty predictions or references list passed to BERTScore.\")\n",
    "        return None\n",
    "    try:\n",
    "        # Ensure predictions and references are paired correctly before filtering\n",
    "        if len(predictions) != len(references):\n",
    "             logger.error(f\"Initial length mismatch for BERTScore: predictions={len(predictions)}, references={len(references)}. Cannot proceed.\")\n",
    "             return None # Or handle this case more gracefully if possible\n",
    "\n",
    "        valid_pairs = [(p, r) for p, r in zip(predictions, references) if p is not None and p.strip() != \"\"]\n",
    "        if not valid_pairs:\n",
    "             logger.warning(\"No valid prediction/reference pairs found after filtering Nones/empty strings.\")\n",
    "             return {'P': [], 'R': [], 'F1': []}\n",
    "\n",
    "        filtered_predictions = [p for p, r in valid_pairs]\n",
    "        filtered_references = [r for p, r in valid_pairs]\n",
    "\n",
    "\n",
    "        if not filtered_predictions: # Should not happen if valid_pairs check passed, but safety check\n",
    "             logger.warning(\"No valid predictions remaining after filtering.\")\n",
    "             return {'P': [], 'R': [], 'F1': []}\n",
    "\n",
    "        bert_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        P, R, F1 = bert_score_calculate(\n",
    "            filtered_predictions,\n",
    "            filtered_references,\n",
    "            model_type=BERT_SCORE_MODEL_TYPE,\n",
    "            lang=\"en\",\n",
    "            verbose=False,\n",
    "            device=bert_device,\n",
    "            batch_size=BATCH_SIZE_BERT_SCORE # Use dedicated BERTScore batch size\n",
    "        )\n",
    "        return {'P': P.tolist(), 'R': R.tolist(), 'F1': F1.tolist()}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating BERTScore: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_file(model_config: ModelConfig, model, tokenizer, input_filepath: str, output_dir: str, device: torch.device):\n",
    "    \"\"\"Processes a single JSON data file using batched generation and adds summary scores.\"\"\"\n",
    "    filename = os.path.basename(input_filepath)\n",
    "    logger.info(f\"Processing file: {filename} for model: {model_config.name} using batch size {GENERATION_BATCH_SIZE}\")\n",
    "\n",
    "    try:\n",
    "        with open(input_filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read or parse JSON file {input_filepath}: {e}\")\n",
    "        return\n",
    "\n",
    "    individual_results = [] # Renamed from 'results' to avoid confusion\n",
    "    all_questions = []\n",
    "    all_ground_truths = []\n",
    "    all_generated_answers = []\n",
    "\n",
    "    # 1. Collect all questions and ground truths first\n",
    "    for item in data:\n",
    "        question = item.get(\"question\")\n",
    "        ground_truth = item.get(\"answer\")\n",
    "        if not question or not ground_truth:\n",
    "            logger.warning(f\"Skipping item due to missing 'question' or 'answer' in {filename}: {item}\")\n",
    "            continue\n",
    "        all_questions.append(question)\n",
    "        all_ground_truths.append(ground_truth)\n",
    "\n",
    "    if not all_questions:\n",
    "        logger.warning(f\"No valid question/answer pairs found in {filename}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # 2. Generate answers in batches\n",
    "    logger.info(f\"Generating answers for {len(all_questions)} questions in batches of {GENERATION_BATCH_SIZE}...\")\n",
    "    num_batches = math.ceil(len(all_questions) / GENERATION_BATCH_SIZE)\n",
    "\n",
    "    for i in tqdm(range(num_batches), desc=f\"Generating Batches - {filename}\"):\n",
    "        batch_start = i * GENERATION_BATCH_SIZE\n",
    "        batch_end = batch_start + GENERATION_BATCH_SIZE\n",
    "        question_batch = all_questions[batch_start:batch_end]\n",
    "        generated_batch = generate_answers_batch(model, tokenizer, question_batch, MAX_NEW_TOKENS)\n",
    "        all_generated_answers.extend(generated_batch)\n",
    "\n",
    "    if len(all_generated_answers) != len(all_questions):\n",
    "         logger.error(f\"Length mismatch after batch generation: questions={len(all_questions)}, generated={len(all_generated_answers)}. Padding with None.\")\n",
    "         all_generated_answers.extend([None] * (len(all_questions) - len(all_generated_answers)))\n",
    "\n",
    "    # 3. Calculate BERTScore in batches\n",
    "    logger.info(f\"Calculating BERTScore for up to {len(all_questions)} pairs...\")\n",
    "    bert_score_device = device\n",
    "    # Pass potentially mismatched lists, calculate_bert_score_batch handles filtering\n",
    "    bert_scores = calculate_bert_score_batch(all_generated_answers, all_ground_truths, bert_score_device)\n",
    "\n",
    "    if bert_scores is None:\n",
    "        logger.error(f\"BERTScore calculation failed for file {filename}. Scores will be null.\")\n",
    "        bert_scores = {'P': [], 'R': [], 'F1': []} # Use empty lists for safety\n",
    "\n",
    "    # 4. Combine individual results and calculate aggregate scores\n",
    "    score_idx = 0\n",
    "    total_p, total_r, total_f1 = 0.0, 0.0, 0.0\n",
    "    valid_score_count = 0\n",
    "    successfully_generated_count = 0\n",
    "\n",
    "    # We need to map scores back to the original items carefully\n",
    "    # Create a list of scores corresponding to the valid generations\n",
    "    scores_p_list = bert_scores.get('P', [])\n",
    "    scores_r_list = bert_scores.get('R', [])\n",
    "    scores_f1_list = bert_scores.get('F1', [])\n",
    "\n",
    "    # Iterate through all original questions\n",
    "    for i in range(len(all_questions)):\n",
    "        question = all_questions[i]\n",
    "        ground_truth = all_ground_truths[i]\n",
    "        generated_answer = all_generated_answers[i]\n",
    "\n",
    "        current_p, current_r, current_f1 = None, None, None\n",
    "\n",
    "        # Check if the answer was generated AND is not empty\n",
    "        is_valid_generation = generated_answer is not None and generated_answer.strip() != \"\"\n",
    "        if is_valid_generation:\n",
    "             successfully_generated_count += 1\n",
    "             # Try to get the score if the index is valid\n",
    "             if score_idx < len(scores_f1_list):\n",
    "                 try:\n",
    "                     current_p = scores_p_list[score_idx]\n",
    "                     current_r = scores_r_list[score_idx]\n",
    "                     current_f1 = scores_f1_list[score_idx]\n",
    "\n",
    "                     # Add to totals for average calculation\n",
    "                     total_p += current_p\n",
    "                     total_r += current_r\n",
    "                     total_f1 += current_f1\n",
    "                     valid_score_count += 1 # Count only items with a valid score triplet\n",
    "\n",
    "                     score_idx += 1 # Move to the next score only if current one was used\n",
    "                 except IndexError:\n",
    "                      logger.error(f\"IndexError accessing BERT scores at index {score_idx} while processing question index {i}. Assigning None.\")\n",
    "                 except TypeError as e: # Handle if scores are not numbers\n",
    "                      logger.error(f\"TypeError accessing BERT score element at index {score_idx}: {e}. Score lists: P={scores_p_list}, R={scores_r_list}, F1={scores_f1_list}. Assigning None.\")\n",
    "\n",
    "             else:\n",
    "                 logger.warning(f\"Score index {score_idx} out of bounds while processing question index {i}. BERT score list length: {len(scores_f1_list)}. Assigning None.\")\n",
    "\n",
    "\n",
    "        result_item = {\n",
    "            # \"model_name\": model_config.name, # Removed, as it's in summary\n",
    "            \"question\": question,\n",
    "            \"ground_truth_answer\": ground_truth,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"bert_score_P\": current_p,\n",
    "            \"bert_score_R\": current_r,\n",
    "            \"bert_score_F1\": current_f1,\n",
    "        }\n",
    "        individual_results.append(result_item)\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_p = total_p / valid_score_count if valid_score_count > 0 else None\n",
    "    avg_r = total_r / valid_score_count if valid_score_count > 0 else None\n",
    "    avg_f1 = total_f1 / valid_score_count if valid_score_count > 0 else None\n",
    "\n",
    "    # Prepare final output data structure\n",
    "    output_data = {\n",
    "        \"summary\": {\n",
    "            \"model_name\": model_config.name,\n",
    "            \"processed_file\": filename,\n",
    "            \"total_questions\": len(all_questions),\n",
    "            \"successfully_generated_count\": successfully_generated_count,\n",
    "            \"scored_item_count\": valid_score_count, # Number of items included in the average\n",
    "            \"average_bert_score_P\": avg_p,\n",
    "            \"average_bert_score_R\": avg_r,\n",
    "            \"average_bert_score_F1\": avg_f1\n",
    "        },\n",
    "        \"details\": individual_results # List of individual results\n",
    "    }\n",
    "\n",
    "\n",
    "    # 5. Save results\n",
    "    model_output_dir = os.path.join(output_dir, model_config.name)\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "    base_filename = os.path.splitext(filename)[0]\n",
    "    output_filename = f\"{base_filename}_results.json\"\n",
    "    output_filepath = os.path.join(model_output_dir, output_filename)\n",
    "\n",
    "    logger.info(f\"Saving results for {filename} to {output_filepath}\")\n",
    "    try:\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "            # Save the structured output_data\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save results to {output_filepath}: {e}\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting model evaluation script (Batched Generation, Quantization Disabled).\")\n",
    "    logger.warning(\"Bitsandbytes quantization is disabled. Ensure you have enough GPU memory.\")\n",
    "    logger.info(f\"Generation Batch Size: {GENERATION_BATCH_SIZE}\")\n",
    "\n",
    "    # Set device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        logger.info(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        logger.info(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # Find all JSON files\n",
    "    all_json_files = []\n",
    "    for data_dir in DATA_DIRECTORIES:\n",
    "        if not os.path.isdir(data_dir):\n",
    "            logger.warning(f\"Data directory not found: {data_dir}. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            for filename in os.listdir(data_dir):\n",
    "                if filename.endswith(\".json\"):\n",
    "                    all_json_files.append(os.path.join(data_dir, filename))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error listing files in directory {data_dir}: {e}\")\n",
    "\n",
    "    if not all_json_files:\n",
    "        logger.error(\"No JSON files found. Exiting.\")\n",
    "        exit()\n",
    "    logger.info(f\"Found {len(all_json_files)} JSON files to process.\")\n",
    "\n",
    "    # Evaluate each model\n",
    "    for model_config in MODEL_CONFIGS:\n",
    "        model = None\n",
    "        tokenizer = None\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            model, tokenizer = load_model_and_tokenizer(model_config, device)\n",
    "            load_time = time.time() - start_time\n",
    "\n",
    "            if model is None or tokenizer is None:\n",
    "                logger.error(f\"Skipping evaluation for model {model_config.name} due to loading failure.\")\n",
    "                continue\n",
    "            logger.info(f\"Model {model_config.name} loaded in {load_time:.2f} seconds.\")\n",
    "\n",
    "            logger.info(f\"--- Starting evaluation for model: {model_config.name} ---\")\n",
    "            for json_filepath in all_json_files:\n",
    "                 process_file(model_config, model, tokenizer, json_filepath, OUTPUT_DIR, device)\n",
    "            logger.info(f\"--- Finished evaluation for model: {model_config.name} ---\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An unexpected error occurred during evaluation of model {model_config.name}: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "        finally:\n",
    "            logger.info(f\"Cleaning up resources for model: {model_config.name}\")\n",
    "            del model, tokenizer\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            logger.info(f\"Resources cleaned up for model: {model_config.name}\")\n",
    "            time.sleep(5) # Add a slightly longer pause\n",
    "\n",
    "    logger.info(\"Script finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
