# evaluate_generated_answers.py

# Standard library imports
import json
import logging
import os
import gc
import time
import traceback
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple, Callable, Any
import math

# Third-party imports
import torch
from tqdm import tqdm
from bert_score import score as bert_score_calculate
from sentence_transformers import SentenceTransformer, util # Added import

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
logging.getLogger("transformers").setLevel(logging.ERROR) # Suppress transformers warnings
logging.getLogger("huggingface_hub").setLevel(logging.ERROR)
logging.getLogger("SentenceTransformer").setLevel(logging.WARNING) # Suppress ST info logs unless warning

# Directory containing the JSON files generated by generate_answers_retry.py
INPUT_GENERATION_DIR = "/scratch/jsong132/De-fine-tuning-Unlearning-Multilingual-Language-Models/Generation/epoch3"
# Directory to save the evaluation results
EVALUATION_OUTPUT_DIR = "/scratch/jsong132/De-fine-tuning-Unlearning-Multilingual-Language-Models/Evaluation/epoch3" # Added _st suffix

# --- Scoring Configuration ---
# Define models for each metric type
BERT_SCORE_MODEL = "bert-base-multilingual-cased"
# Choose a Sentence Transformer model suitable for semantic similarity
# 'paraphrase-multilingual-mpnet-base-v2' is good for multiple languages
# 'all-mpnet-base-v2' is excellent for English
ST_MODEL_NAME = 'paraphrase-multilingual-mpnet-base-v2'

# Define which scoring functions to use
# Each key is the metric prefix, value is the calculation function (using lambda to pass config)
SCORING_FUNCTIONS = {
    "bert_score": lambda preds, refs, device: calculate_bert_scores(
        preds, refs, device, model_type=BERT_SCORE_MODEL, batch_size=16
    ),
    "st_similarity": lambda preds, refs, device: calculate_st_cosine_similarity(
        preds, refs, device, model_name=ST_MODEL_NAME
    ),
    # Add more scoring functions here later
}
DEFAULT_LANG_FOR_BERT_SCORE = "en"

# Global variable to cache the Sentence Transformer model (optional optimization)
# Set to None initially. It will be loaded when first needed.
_st_model_cache: Optional[SentenceTransformer] = None


# --- Scoring Functions ---

def calculate_bert_scores(
    predictions: List[Optional[str]],
    references: List[Optional[str]],
    device: torch.device,
    model_type: str,
    batch_size: int = 16,
    lang: Optional[str] = DEFAULT_LANG_FOR_BERT_SCORE
) -> Tuple[List[int], Dict[str, List[float]]]:
    """Calculates BERTScore (P, R, F1) for valid pairs (Identical to previous version)."""
    if not predictions or not references or len(predictions) != len(references):
        logger.error(f"Invalid input lists for BERTScore: preds({len(predictions)}), refs({len(references)})")
        return [], {'P': [], 'R': [], 'F1': []}

    valid_pairs = []
    original_indices = []
    for i, (pred, ref) in enumerate(zip(predictions, references)):
        if pred is not None and isinstance(pred, str) and pred.strip() and \
           ref is not None and isinstance(ref, str) and ref.strip():
            valid_pairs.append((pred, ref))
            original_indices.append(i)

    if not valid_pairs:
        logger.warning("No valid prediction/reference pairs found for BERTScore calculation.")
        return [], {'P': [], 'R': [], 'F1': []}

    filtered_predictions = [p for p, r in valid_pairs]
    filtered_references = [r for p, r in valid_pairs]

    logger.info(f"Calculating BERTScore ({model_type}) for {len(filtered_predictions)} valid pairs...")
    bert_device = 'cuda' if torch.cuda.is_available() else 'cpu'

    try:
        P, R, F1 = bert_score_calculate(
            filtered_predictions, filtered_references, model_type=model_type,
            lang=lang, verbose=False, device=bert_device, batch_size=batch_size
        )
        logger.info("BERTScore calculation finished.")
        return original_indices, {'P': P.tolist(), 'R': R.tolist(), 'F1': F1.tolist()}
    except Exception as e:
        logger.error(f"Error calculating BERTScore: {e}", exc_info=True) # Log traceback
        return [], {'P': [], 'R': [], 'F1': []}


def calculate_st_cosine_similarity(
    predictions: List[Optional[str]],
    references: List[Optional[str]],
    device: torch.device,
    model_name: str
) -> Tuple[List[int], Dict[str, List[float]]]:
    """
    Calculates Sentence Transformer cosine similarity for valid pairs.

    Args:
        predictions: List of generated answers (can contain None).
        references: List of ground truth answers (can contain None).
        device: The torch device ('cuda' or 'cpu').
        model_name: The Sentence Transformer model name.

    Returns:
        A tuple containing:
        - original_indices: List of indices from the *original* input lists
                          corresponding to the calculated scores.
        - scores: A dictionary {'cosine_similarity': [scores...]}
                  containing the scores only for the valid pairs.
                  Returns empty lists if no valid pairs are found or on error.
    """
    global _st_model_cache # Access the global cache

    if not predictions or not references or len(predictions) != len(references):
        logger.error(f"Invalid input lists for ST Similarity: preds({len(predictions)}), refs({len(references)})")
        return [], {'cosine_similarity': []}

    valid_pairs = []
    original_indices = []
    for i, (pred, ref) in enumerate(zip(predictions, references)):
        if pred is not None and isinstance(pred, str) and pred.strip() and \
           ref is not None and isinstance(ref, str) and ref.strip():
            valid_pairs.append((pred, ref))
            original_indices.append(i)

    if not valid_pairs:
        logger.warning("No valid prediction/reference pairs found for ST Similarity calculation.")
        return [], {'cosine_similarity': []}

    filtered_predictions = [p for p, r in valid_pairs]
    filtered_references = [r for p, r in valid_pairs]

    logger.info(f"Calculating Sentence Transformer Similarity ({model_name}) for {len(filtered_predictions)} valid pairs...")

    try:
        # Load model if not cached or if device changed (unlikely but safe)
        if _st_model_cache is None or str(_st_model_cache.device) != str(device):
            logger.info(f"Loading Sentence Transformer model: {model_name} onto device: {device}")
            _st_model_cache = SentenceTransformer(model_name, device=device)
            logger.info("Sentence Transformer model loaded.")
        else:
            logger.info("Using cached Sentence Transformer model.")

        st_model = _st_model_cache

        # Encode sentences - encode predictions and references separately for pairwise comparison
        logger.debug("Encoding predictions...")
        embeddings_pred = st_model.encode(filtered_predictions, convert_to_tensor=True, show_progress_bar=False)
        logger.debug("Encoding references...")
        embeddings_ref = st_model.encode(filtered_references, convert_to_tensor=True, show_progress_bar=False)

        # Calculate pairwise cosine similarity
        # util.cos_sim returns a matrix, we need the diagonal for pred[i] vs ref[i]
        cosine_matrix = util.cos_sim(embeddings_pred, embeddings_ref)
        similarity_scores = torch.diag(cosine_matrix).tolist() # Get the diagonal elements

        logger.info("Sentence Transformer Similarity calculation finished.")
        return original_indices, {'cosine_similarity': similarity_scores}

    except Exception as e:
        logger.error(f"Error calculating Sentence Transformer Similarity: {e}", exc_info=True) # Log traceback
        return [], {'cosine_similarity': []}


# --- calculate_scores Function (Handles multiple metrics - unchanged) ---
def calculate_scores(
    predictions: List[Optional[str]],
    references: List[Optional[str]],
    scoring_functions: Dict[str, Callable],
    device: torch.device
) -> Tuple[Dict[int, Dict[str, float]], Dict[str, float]]:
    """Calculates multiple scores using the provided scoring functions."""
    detailed_scores: Dict[int, Dict[str, float]] = {}
    aggregate_totals: Dict[str, float] = {}
    valid_counts: Dict[str, int] = {}

    for prefix, func in scoring_functions.items():
        logger.info(f"--- Calculating scores for metric: {prefix} ---")
        try:
            original_indices, current_scores = func(predictions, references, device)

            if not original_indices or not current_scores:
                logger.warning(f"No scores returned for metric: {prefix}")
                continue

            for score_name, score_list in current_scores.items():
                full_score_name = f"{prefix}_{score_name}" # e.g., st_similarity_cosine_similarity
                aggregate_totals[full_score_name] = 0.0
                valid_counts[full_score_name] = 0

                if len(original_indices) != len(score_list):
                    logger.error(f"Index/Score list length mismatch for {full_score_name}. Skipping.")
                    continue

                for i, score_value in enumerate(score_list):
                    original_idx = original_indices[i]
                    if original_idx not in detailed_scores:
                        detailed_scores[original_idx] = {}

                    if isinstance(score_value, (float, int)): # Ensure it's a number
                        detailed_scores[original_idx][full_score_name] = float(score_value)
                        aggregate_totals[full_score_name] += float(score_value)
                        valid_counts[full_score_name] += 1
                    else:
                         detailed_scores[original_idx][full_score_name] = None # Store None if invalid

        except Exception as e:
            logger.error(f"Failed to calculate scores for metric {prefix}: {e}", exc_info=True)

    average_scores = {}
    for score_name, total in aggregate_totals.items():
        count = valid_counts.get(score_name, 0)
        avg_score_name = f"average_{score_name}"
        average_scores[avg_score_name] = total / count if count > 0 else None

    return detailed_scores, average_scores


# --- process_evaluation_file Function (Adds scores to output - minor update for clarity) ---
def process_evaluation_file(input_filepath: str, output_dir: str, scoring_functions: Dict[str, Callable], device: torch.device):
    """Loads generated results, calculates scores, and saves evaluated results."""
    filename = os.path.basename(input_filepath)
    model_name = os.path.basename(os.path.dirname(input_filepath))
    logger.info(f"Evaluating file: {filename} for model: {model_name}")

    try:
        with open(input_filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        logger.error(f"Failed to read or parse JSON file {input_filepath}: {e}", exc_info=True)
        return

    if "details" not in data or not isinstance(data["details"], list):
        logger.error(f"Invalid format: 'details' list not found in {input_filepath}")
        return

    details = data["details"]
    summary = data.get("summary", {})
    summary["model_name"] = model_name

    all_predictions = [item.get("generated_answer") for item in details]
    all_references = [item.get("ground_truth_answer") for item in details]

    detailed_scores, average_scores = calculate_scores(
        all_predictions, all_references, scoring_functions, device
    )

    # Add scores to the details items
    valid_indices_scored = set(detailed_scores.keys())
    for i, item in enumerate(details):
        if i in detailed_scores:
            item.update(detailed_scores[i])
        # Optionally ensure all possible score fields exist (set to None if not calculated)
        # This part might become complex if many metrics are added.
        # It might be sufficient to rely on the detailed_scores dict having the relevant keys.

    # Update summary
    summary["valid_pairs_for_scoring"] = len(valid_indices_scored)
    summary.update(average_scores)

    # Prepare final output data structure
    output_data = {
        "summary": summary,
        "details": details
    }

    # Save evaluated results
    model_output_dir = os.path.join(output_dir, model_name)
    os.makedirs(model_output_dir, exist_ok=True)
    base_filename = os.path.splitext(filename)[0].replace("_generated_retry", "")
    output_filename = f"{base_filename}_evaluated.json"
    output_filepath = os.path.join(model_output_dir, output_filename)

    logger.info(f"Saving evaluation results for {filename} to {output_filepath}")
    try:
        with open(output_filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
    except Exception as e:
        logger.error(f"Failed to save evaluation results to {output_filepath}: {e}", exc_info=True)


# --- Main Execution (Mostly unchanged, uses new output dir name) ---
if __name__ == "__main__":
    logger.info("Starting evaluation script for generated answers (with Sentence Transformer).")

    # Set device
    if torch.cuda.is_available():
        device = torch.device("cuda")
        logger.info(f"CUDA available. Using GPU for potential scoring: {torch.cuda.get_device_name(0)}")
    else:
        device = torch.device("cpu")
        logger.info("CUDA not available. Using CPU for potential scoring.")

    if not os.path.isdir(INPUT_GENERATION_DIR):
        logger.error(f"Input directory not found: {INPUT_GENERATION_DIR}")
        exit(1)

    os.makedirs(EVALUATION_OUTPUT_DIR, exist_ok=True) # Uses the _st directory

    # Find files (same logic)
    files_to_evaluate = []
    for model_dir in os.listdir(INPUT_GENERATION_DIR):
        model_path = os.path.join(INPUT_GENERATION_DIR, model_dir)
        if os.path.isdir(model_path):
            try:
                for filename in os.listdir(model_path):
                    if filename.endswith("_generated_retry.json"):
                        files_to_evaluate.append(os.path.join(model_path, filename))
            except Exception as e:
                logger.error(f"Error listing files in directory {model_path}: {e}")

    if not files_to_evaluate:
        logger.error(f"No '*_generated_retry.json' files found in subdirs of {INPUT_GENERATION_DIR}. Exiting.")
        exit()

    logger.info(f"Found {len(files_to_evaluate)} generated files to evaluate.")
    logger.info(f"Configured metrics: {list(SCORING_FUNCTIONS.keys())}")

    # Evaluate each file
    overall_start_time = time.time()
    for json_filepath in tqdm(files_to_evaluate, desc="Evaluating Files"):
        file_start_time = time.time()
        process_evaluation_file(json_filepath, EVALUATION_OUTPUT_DIR, SCORING_FUNCTIONS, device)
        file_end_time = time.time()
        # logger.info(f"Processed {os.path.basename(json_filepath)} in {file_end_time - file_start_time:.2f} seconds.")
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # Clear the model cache at the end (optional)
    _st_model_cache = None
    gc.collect()
    if torch.cuda.is_available():
            torch.cuda.empty_cache()

    overall_end_time = time.time()
    logger.info(f"Evaluation script finished in {overall_end_time - overall_start_time:.2f} seconds.")